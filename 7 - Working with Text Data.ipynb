{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "review_train = load_files('data/aclImdb/train')\n",
    "text_train, y_train = review_train.data, review_train.target\n",
    "\n",
    "len(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = load_files('data/aclImdb/test')\n",
    "text_test, y_test = review_test.data, review_test.target\n",
    "\n",
    "len(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing <br /> tags from the datasets\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Gloomy Sunday - Ein Lied von Liebe und Tod directed by Rolf Sch\\xc3\\xbcbel in 1999 is a romantic, absorbing, beautiful, and heartbreaking movie. It started like Jules and Jim; it ended as one of Agatha Christie\\'s books, and in between it said something about love, friendship, devotion, jealousy, war, Holocaust, dignity, and betrayal, and it did better than The Black Book which is much more popular. It is not perfect, and it made me, a cynic, wonder in the end on the complexity of the relationships and sensational revelations, and who is who to whom but the movie simply overwhelmed me. Perfect or not, it is unforgettable. All four actors as the parts of the tragic not even a triangle but a rectangle were terrific. I do believe that three men could fell deeply for one girl as beautiful and dignified as Ilona in a star-making performance by young Hungarian actress Erica Marozs\\xc3\\xa1n and who would not? The titular song is haunting, sad, and beautiful, and no doubt deserves the movie been made about it and its effect on the countless listeners. I love the movie and I am surprised that it is so little known in this country. It is a gem.  The fact that it is based on a story of the song that had played such important role in the lives of all characters made me do some research, and the real story behind the song of Love and Death seems as fascinating as the fictional one. The song was composed in 1930s by Rezs\\xc3\\xb6 Seress and was believed to have caused many suicides in Hungary and all over Europe as the world was moving toward the most devastating War of the last century. Rezs\\xc3\\xb6 Seress, a Jewish-Hungarian pianist and composer, was thrown to the Concentration Camp but survived, unlike his mother. In January, 1968, Seress committed suicide in Budapest by jumping out of a window. According to his obituary in the New York Times, \"Mr. Seres complained that the success of \"Gloomy Sunday\" actually increased his unhappiness, because he knew he would never be able to write a second hit.\"   Many singers from all over the world have recorded their versions of the songs in different languages. Over 70 performers have covered the song since 1935, and some famous names include Billie Holiday, Paul Robeson, Pyotr Leschenko (in Russian, under title \"Mratschnoje Woskresenje\"), Bjork, Sarah McLachlan, and many more. The one that really got to me and made me shiver is by Diamanda Gal\\xc3\\xa1s, the Greek born American singer/pianist/performer with the voice of such tragic power that I still can\\'t get over her singing. Gal\\xc3\\xa1s has been described as \"capable of the most unnerving vocal terror\", and in her work she mostly concentrates on the topics of \"suffering, despair, condemnation, injustice and loss of dignity.\" When she sings the Song of Love and Death, her voice that could\\'ve belonged to the most tragic heroines of Ancient Greece leaves no hope and brings the horror and grief of love lost forever to the unbearable and incomparable heights.  8.5/10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 13\n",
      "Vocabulary Content:\n",
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n",
      "Bag of Words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# toy dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = ['The fool doth think he is wise,', 'but the wise man knows himself to be a fool']\n",
    "vect = CountVectorizer().fit(bards_words)\n",
    "\n",
    "print('Vocabulary Size: {}'.format(len(vect.vocabulary_)))\n",
    "print('Vocabulary Content:\\n{}'.format(vect.vocabulary_))\n",
    "\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "print('Bag of Words:\\n{}'.format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 124255\n",
      "First 20 features:\n",
      "['00', '000', '0000', '0000000000000000000000000000000001', '0000000000001', '000000001', '000000003', '00000001', '000001745', '00001', '0001', '00015', '0002', '0007', '00083', '000ft', '000s', '000th', '001', '002']\n",
      "Every 2000th feature:\n",
      "['00', '_require_', 'aideed', 'announcement', 'asteroid', 'banquière', 'besieged', 'bollwood', 'btvs', 'carboni', 'chcialbym', 'clotheth', 'consecration', 'cringeful', 'deadness', 'devagan', 'doberman', 'duvall', 'endocrine', 'existent', 'fetiches', 'formatted', 'garard', 'godlie', 'gumshoe', 'heathen', 'honoré', 'immatured', 'interested', 'jewelry', 'kerchner', 'köln', 'leydon', 'lulu', 'mardjono', 'meistersinger', 'misspells', 'mumblecore', 'ngah', 'oedpius', 'overwhelmingly', 'penned', 'pleading', 'previlage', 'quashed', 'recreating', 'reverent', 'ruediger', 'sceme', 'settling', 'silveira', 'soderberghian', 'stagestruck', 'subprime', 'tabloids', 'themself', 'tpf', 'tyzack', 'unrestrained', 'videoed', 'weidler', 'worrisomely', 'zombified']\n"
     ]
    }
   ],
   "source": [
    "# movie review dataset\n",
    "\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print('Number of features: {}'.format(len(feature_names)))\n",
    "print('First 20 features:\\n{}'.format(feature_names[:20]))\n",
    "print('Every 2000th feature:\\n{}'.format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 44532\n",
      "First 20 features:\n",
      "['00', '000', '001', '007', '00am', '00pm', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '1001']\n",
      "Every 2000th feature:\n",
      "['00', 'anxiety', 'bevy', 'capitalists', 'compliance', 'deck', 'drilling', 'extinguished', 'gals', 'haute', 'ineffectually', 'knifed', 'malcomb', 'morrow', 'ott', 'policies', 'rebuffed', 'sadder', 'sir', 'strick', 'tinkles', 'uprising', 'wreaks']\n"
     ]
    }
   ],
   "source": [
    "# using min_df\n",
    "\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print('Number of features: {}'.format(len(feature_names)))\n",
    "print('First 20 features:\\n{}'.format(feature_names[:20]))\n",
    "print('Every 2000th feature:\\n{}'.format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stopwords: 318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print('Number of Stopwords: {}'.format(len(ENGLISH_STOP_WORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 44223\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words='english').fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print('Number of features: {}'.format(len(feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'one' 'be' 'have']\n",
      "Features with highest tfidf:\n",
      "['donegal' 'hotheaded' 'donatello' 'rylance' 'ruptured' 'téa' 'tz'\n",
      " 'tyzack' 'domini' 'hpl' 'ryo' 'hoyts' 'asbestos' 'hoverboy' 'bunks'\n",
      " 'housesitter' 'ruts' 'rw' 'russells' 'shipments']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vectorizer.transform(text_train)\n",
    "\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tdidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print('Features with lowest tfidf:\\n{}'.format(feature_names[sorted_by_tfidf[:20]]))\n",
    "print('Features with highest tfidf:\\n{}'.format(feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'one' 'be' 'have' 'are' 'film' 'you' 'all'\n",
      " 'at' 'an' 'by' 'from' 'so' 'like' 'who' 'there' 'they' 'his' 'if' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'can' 'good' 'when' 'more'\n",
      " 'up' 'time' 'very' 'even' 'only' 'no' 'see' 'would' 'my' 'story' 'really'\n",
      " 'which' 'well' 'had' 'me' 'than' 'their' 'much' 'were' 'get' 'other' 'do'\n",
      " 'been' 'most' 'also' 'into' 'don' 'her' 'first' 'great' 'how' 'made'\n",
      " 'people' 'will' 'make' 'because' 'way' 'could' 'bad' 'we' 'after' 'them'\n",
      " 'too' 'any' 'then' 'movies' 'watch' 'she' 'think' 'seen' 'acting' 'its'\n",
      " 'characters']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "\n",
    "print('Features with lowest idf:\\n{}'.format(feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-Grams (Bag-of-Words with More than 1 word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 14\n",
      "Vocabulary Content:\n",
      "{'the fool': 9, 'fool doth': 3, 'doth think': 2, 'think he': 11, 'he is': 4, 'is wise': 6, 'but the': 1, 'the wise': 10, 'wise man': 13, 'man knows': 8, 'knows himself': 7, 'himself to': 5, 'to be': 12, 'be fool': 0}\n"
     ]
    }
   ],
   "source": [
    "# bigrams\n",
    "# two tokens following each-other (2,2)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = ['The fool doth think he is wise,', 'but the wise man knows himself to be a fool']\n",
    "vect = CountVectorizer(ngram_range=(2,2)).fit(bards_words)\n",
    "\n",
    "print('Vocabulary Size: {}'.format(len(vect.vocabulary_)))\n",
    "print('Vocabulary Content:\\n{}'.format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 39\n",
      "Vocabulary Content:\n",
      "{'the': 25, 'fool': 8, 'doth': 5, 'think': 30, 'he': 11, 'is': 17, 'wise': 36, 'the fool': 26, 'fool doth': 9, 'doth think': 6, 'think he': 31, 'he is': 12, 'is wise': 18, 'the fool doth': 27, 'fool doth think': 10, 'doth think he': 7, 'think he is': 32, 'he is wise': 13, 'but': 2, 'man': 22, 'knows': 19, 'himself': 14, 'to': 33, 'be': 0, 'but the': 3, 'the wise': 28, 'wise man': 37, 'man knows': 23, 'knows himself': 20, 'himself to': 15, 'to be': 34, 'be fool': 1, 'but the wise': 4, 'the wise man': 29, 'wise man knows': 38, 'man knows himself': 24, 'knows himself to': 21, 'himself to be': 16, 'to be fool': 35}\n"
     ]
    }
   ],
   "source": [
    "# trigrams\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,3)).fit(bards_words)\n",
    "\n",
    "print('Vocabulary Size: {}'.format(len(vect.vocabulary_)))\n",
    "print('Vocabulary Content:\\n{}'.format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "['-PRON-', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', '-PRON-', 'be', 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n",
      "Stemming:\n",
      "['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', 'am', 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def compare_normalization(doc):\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    print('Lemmatization:\\n{}'.format([token.lemma_ for token in doc_spacy]))\n",
    "    print('Stemming:\\n{}'.format([stemmer.stem(token.norm_.lower()) for token in doc_spacy]))\n",
    "    \n",
    "compare_normalization(u\"Our meeting today was worse than yesterday, \"\"I'm scared of meeting the clients tomorrow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
